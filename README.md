# Cloud-Data-Engineering-Hackathon

This repository showcases the project I developed as part of the **SMIT Cloud Data Engineering Course**.  
It features a fully serverless, real-time data ingestion and processing architecture using AWS services, handling data from multiple live sources and integrating it into cloud data stores.

---

## ğŸš€ Project Overview

A robust, production-grade pipeline for ingesting, processing, and storing:

- ğŸ“ˆ Minute-level stock data from **Yahoo Finance**
- ğŸ’° Live cryptocurrency snapshots from **CoinMarketCap**
- ğŸ’± Real-time forex rates from **Open Exchange Rates**

All workflows are designed to be **automated**, **scalable**, and **cloud-native**, simulating real-world big data platforms.

---

## ğŸ“Œ Project Objective

Build a real-time event-driven data engineering solution that:

- Supports multi-source ingestion on a 1-minute schedule
- Stores raw and processed data in S3, Snowflake, and SQL Server
- Uses Lambda functions for transformation logic
- Uses FIFO queues for guaranteed ordering
- Runs entirely in a serverless AWS environment

---

## ğŸ› ï¸ Technologies Used

| Category        | Tools / Services |
|----------------|------------------|
| Cloud Platform | AWS (Lambda, S3, SQS, SNS, EventBridge, IAM) |
| Programming    | Python (boto3, yfinance, BeautifulSoup, requests) |
| Storage        | Amazon S3, SQL Server, â„ï¸ Snowflake |
| Messaging      | Amazon SQS (FIFO), Amazon SNS |
| Scheduling     | Amazon EventBridge (1-minute triggers) |
| Deployment     | Ngrok (for exposing local SQL Server) |
| Packaging      | AWS Lambda Layers (for native Python dependencies) |

---

## ğŸ§± System Architecture

### ğŸ“¥ 1. Ingestion Layer

Each data source has a dedicated AWS Lambda function triggered every minute via EventBridge.

| Source             | Description |
|--------------------|-------------|
| Yahoo Finance      | Uses `yfinance` to fetch OHLCV for S&P 500 |
| CoinMarketCap      | Scrapes top 10 cryptocurrencies using BeautifulSoup |
| Open Exchange Rates| Pulls forex rates via secure API |

Data is stored in this structured format on S3:
s3://<bucket>/raw/<source>/YYYY/MM/DD/HHMM.csv

---

### ğŸ“¨ 2. Event Notifications

When new files are added to S3:

- An **SNS** notification is published
- **SNS** uses message attributes (like `source`) to fan-out to:
  - `yahoo-finance-queue.fifo`
  - `coinmarketcap-queue.fifo`
  - `openexchangerates-queue.fifo`

---

### ğŸ”„ 3. Processing Layer

Each queue has a dedicated Lambda consumer which:

- Downloads the respective file from S3
- Parses and filters the data (e.g., status == success)
- Transforms the records
- Inserts into appropriate data sinks:

| Source             | Target Sink   |
|--------------------|---------------|
| Yahoo Finance      | â„ï¸ Snowflake |
| CoinMarketCap      | ğŸ—‚ï¸ Processed S3 |
| Open Exchange Rates| ğŸ§® SQL Server |

---

## ğŸ”’ SQL Server Access via Ngrok

Since AWS Lambda functions cannot access local services by default, I used ngrok to publicly expose a locally hosted SQL Server database. This allowed real-time insertions from Lambda without needing complex VPC setups.
    ```bash
    ngrok tcp 1433
    
---

## ğŸ“¦ Lambda Layer Packaging

Creating Lambda Layers was one of the more challenging parts due to native Python package dependencies. Here's how it was done:

### ğŸ§ª Steps

1. Launch an **EC2 instance** using Amazon Linux 2023.
2. Install required packages using `pip`, including:
   - `pandas`
   - `beautifulsoup4`
   - `yfinance`
3. Navigate to the `site-packages` directory inside the virtual environment.
4. Zip the contents of `site-packages`:
   ```bash
   cd <path-to-site-packages>
   zip -r9 lambda-layer.zip .



